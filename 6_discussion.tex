\section{Discussion}\label{discussion}

The goal of the study is to explore design opportunities to support
collaboration in intelligence analysis by evaluating tool usage in a natural
environment over multiple usage sessions. Our work builds upon prior
empirical studies (e.g.
\cite{Carroll2013,Borge2012,Kang2011,Chin2009}) and embodies
their design implications in our tool. The study also complements
research that only tests tools in short term lab studies (e.g.
\cite{Convertino2011,Goyal2016,Hajizadeh2013}) by investigating tool usage over multiple usage sessions.

This study adopts an \emph{analyst-centered design} approach. A critical requirement of developing tools that meet user needs is to
understand their needs and practices. When these needs and practices
are specialized (as is the case in Intelligence Analysis), it is
particularly important to include the target user population in
the design process \cite{Scholtz2014}.
Our classroom study provided an opportunity with deep
access to analysts in training. These analysts have been trained
with knowledge and skills of intelligence analysis, and have experience with state-of-the-art analytic
tools such as Analyst's Notebook and PARC ACH. In their reflections, participants often compared CAnalytics to
those tools. Their multi-session usage of CAnalytics also allows them to adapt to the tool and learn to appropriate it to the best of their team purpose \cite{Stahl2006}. Therefore, while their feedback is
admittedly not the same as an experienced professional,
their feedback does provide a deeper insight into strengths and
weaknesses of CAnalytics.

The study provides encouraging results on supporting collaboration in intelligence analysis.
Participants appreciated an all-in-one environment where they could
share raw documents, evidence snippets, views of evidence and hypotheses
in one place. They liked the fact that they could contribute
simultaneously without blocking or interfering each other. Another
benefit of the collaborative tool is to keep teammates aware of each
other's activities. Staying aware of teammates not only helps establish a common ground for planning team strategy, but also ensure everyone is following the plan as expected. Moreover, results suggest the awareness features provide positive \emph{social facilitation} \cite{Zajonc1965}: individuals found the task motivating and engaging with awareness of each other's activity. We also measured collaboration characteristics that impacted team performance, and found that balanced contribution, peer editing, and earlier switching  from modeling to analysis predicted higher team performance.

\paragraph{Scaffold a structured interleaving workflow}

A misconception about information analysis is that data modeling and
data analysis are two staged activities. This is akin to the \emph{waterfall}
software development model, which features a sequential process that
flows downwards through the phases of requirement conception, software
design and implementation, and testing and maintenance. Critics have
argued against this approach and instead called for an iterative design
process that leads to reframe user requirements,
redesign, redevelopment, and retesting.

Our result demonstrates a similar iterative and dynamic process in intelligence
analysis. The result is striking especially because our participants have been
trained with tools that impose a waterfall model. They could have followed their
old waterfall practice with our tool, yet instead all teams spontaneously
switched to an iterative manner. Our result also suggests that an earlier switch
to analysis actually increases team performance. Realizing that, we probably
could shape analysts into a more structured, interleaving workflow. The structured techniques such as IEW and ACH aim to scaffold analysts to a systematic approach to build models and validate hypotheses respectively. Similarly, the system can provide a scaffolding process to assist analysts in connecting data and analysis. For example, when user adds a new data object, the system could
suggest possible connections to existing evidence, which is likely to help analysts discover new patterns. When a user creates a new hypothesis, the system could highlight associated evidence, which would prompt the analyst to re-examine the data and look for more data. Note though that this scaffolding should only provide a basic structure rather than imposing analysts a fixed workflow \cite{Kang2011}.


\paragraph{Design a richer graphic language for uncertainty}

Teams need to be aware of not only the content of shared information, but also
its uncertainty. Uncertainty is prevalent in team analysis because each teammate becomes a source of uncertainty when they contribute a piece of information. Teams in our study spontaneously employed two different approaches to
deal with uncertainty: either to mix them for better synthesis or to
separate them for better clarification. Representing uncertainty appropriately assures teammates when they build analysis upon other's insight. This helps reduce the problem emphasized in Chin et al.'s work \cite{Chin2009} that analysts do not trust partner's conclusion. We propose that a richer graphic
language and interaction be designed so that analysts can encode uncertainty
into the views. For example, links and entities with different
uncertainty can be visualized in different transparency. Users can \emph{filter}
by uncertainty so that users can choose to consider evidence only with high credibility, or to review
all inferences and decide what extra data is needed to consolidate them.

\paragraph{Distinguish visible vs. valuable contributions}

We noted cases where teams created far more entities than needed
with an accretion strategy.
Strikingly, while similar data modeling strategy was reported in the paper prototype
study \cite{Carroll2013}, users with CAnalytics seemed far more
tempted to accretively add information, with far more entities and
cluttered views. For example, the extreme team created as many as three times entities than the rest teams in our study, much more than the difference in the paper prototype study. Why did this
happen? We guess both the context of classroom study and the system
design contributed. Unlike in the lab study where teams are temporarily
assembled, teams in a class evaluate peers either consciously or
unconsciously and value how themselves are being evaluated. Such social
pressure motivate individuals to make contributions, and indeed to make
\emph{visible} contributions, more than \emph{valuable} contributions. That is,
participants noticed that their work activity was visible to their
partners, and accordingly prioritized doing more visible work over doing
less visible work. In some cases, this led to a new problem of easy and
less valuable contributions that were highly visible - such as creating
and therefore sharing data entities that were not particularly
important, and subsequently made data models seem cluttered. For
example, creating and therefore sharing an entity gets immediately
notified to the team whereas weighing the importance and relevance of an
entity goes silent in the system. We need to investigate approaches to
making significant contributions more visible, or perhaps making it more
immediately visible that less important contributions are indeed less
important.


\paragraph{Share views as team resource}

View sharing is important for sharing and understanding analytic result \cite{Morton2014a}. A common solution is to enable a public view which always keeps synchronized for all teammates \cite{Convertino2011,Greenberg1990}. However, a single public view does not meet the need in intelligence analysis because analysts may want to share multiple views in an iterative analysis process.
We propose that views should be treated as a
\emph{team resource}, just like data, which is sharable,
extensible, and reusable. For example, an analyst can save their current view as a shared resource when
they feel it useful to collaborators. Other people can reuse the view
to their need. Shared views should be interactive rather than static
images, so that analysts can perform all interactions including
filtering and highlighting, and are able to evolve the view with
collective team efforts, a critical requirement emphasized in
\cite{Carroll2013}.


\paragraph{Build collapsible views for multi-level analysis}

we observed that analysts
frequently engaged in multi-level analysis,
and coordinate among different levels of details. For example, analysts jumped quickly from digging into details of a single event, to
comparing between two events, and to overviewing all robberies as a
complete story. When sharing an insight that associate This brings challenges to information sharing because information should not only be delivered to partners but should also be represented at the right context so that partners could understand it in the right context. A collapsible data view could be a solution to accommodate such multi-level analysis. This can help draw teammate's attention to a specific item while keeping the global context available. Analysts can focus on a certain level of
detail at a time while conveniently switching between levels. A collapsible view also reduces the problem of cluttered view when data volume increases, as some participants complained.  and when analysts
dig into greater details (e.g.~representing suspect's all actions to
identify patterns of common actions in two robberies). An analyst can overview all robberies and only unfold detailed actions when investigating into a specific robbery.
