
\section{Discussion}\label{discussion}

\subsection{Reflections on method}\label{reflections-on-method}

The goal of the study is to explore design opportunities to support
collaborative information analysis by evaluating tool usage in a natural
environment over multiple usage sessions. Our work builds upon prior
empirical studies (e.g.
\autocites{Carroll2013}{Borge2012}{Kang2011}{Chin2009}) and embodies
their design implications in our tool. We are also to complement
research that only tests tools in short term lab studies (e.g.
\autocites{Convertino2011}{Goyal2016}{Hajizadeh2013}). Due to the
constraint of time (mostly within one hour), these studies had to employ
a simplified task with reduced content and complexity. Participants
would thus have no need to create complex information artifacts
(e.g.~with a single artifact or few items in an artifact). More complex
task would push participants to create more sophisticated artifacts
(e.g.~multiple views or cluttered display that requires filtering) and
to balance between team coordination and individual analysis, which
would have provided more insights into team-based analytic process.

Our classroom study attempts to gain deeper insights on collaborative
information analysis behavior by better simulating real world settings
in two aspects:

First, the study spanned multiple usage sessions over one-week long.
Teams are able to learn to adapt to team functions and to appropriate
the tool to best serve their team purpose \autocite{Stahl2006}. They can
explore different strategies in seeking to solve the problem, and to
make changes if they get stuck with one strategy. For example, we
noticed that two teams decided to change the use of the tool halfway in
their analysis. One team started with dividing work by case documents,
but later decided members should annotate different entity types.
Another team started with an accretion strategy by annotating all
entities. Later they discovered that this strategy brought too much
noise, and decided to clean out irrelevant entities (filtering
strategy). Such change occurs as a consequence of increased awareness of
team functions and tool capabilities, which takes time to develop.

Second, participants in this study are being trained to become
professional analysts. Before our study they had already been introduced
to the information analysis techniques and the state-of-the-art analytic
tools. In their reflections, participants often compared CAnalytics to
those tools, as well as the different teamwork experiences. Therefore
their feedback is likely to provide deeper insight into strength and
weakness of CAnalytics. In addition, the students are young learners
that are willing to employ new work practices supported by features in
tools. They are important parts of the future analytic community. In
some sense, their practice can be treated as a view into the future of
practice of the community \autocite{Olson2017}.

Yet classroom study also has limits. For example, many factors and
variables could exist that affect team performance. The fact that these
factors are often impossible to model or control adds to the difficulty
in data analysis (e.g.~teammate absence). Also, data collection is
challenging because team interactions are not always accessible. Teams
can choose to work synchronously or asynchronously, and it is difficult
to predict when or where the interaction of most interest is to occur.
Verbal communication is not accessible, which could be useful to infer
team awareness as a complement to interaction logs. Our work identifies
both positive evidence and problematic situations, and propose potential
solutions and possible hypotheses, yet rigorously evaluating these
solutions and validating hypotheses is beyond this study. Lab studies
and case studies can be conducted in the future to address these
problems with greater control and deeper data access.

\subsection{Reflections on result}\label{reflections-on-result}

The study provides encouraging results on supporting collaborative
information analysis with an integrated workspace and awareness design.
Participants appreciated an all-in-one environment where they could
share raw documents, evidence snippets, views of evidence and hypotheses
in one place. They liked the fact that they could contribute
simultaneously without blocking or interfering each other. Another
benefit of the collaborative tool is to keep teammates aware of each
other's activities. In addition to the many aspects of awareness listed
in Table \autocite{tab:awareness}, which help establish a common ground
for plan execution, obtaining real time feedback and staying aware that
teammates are indeed following the team plan is equally important.
Moreover, participants suggest the awareness features in the tool have
positive social impact such as \emph{social facilitation} , whilst
CAnalytics provides an environment where individuals simultaneously
engage in the same activity in awareness of each other (co-action
effect) and the notification and other awareness features act as a
source of arousal \autocite{Zajonc1965}.

Our study suggests that tools play an important role in shaping user's
behavior towards more collaborative behavior. With traditional
single-user tools, students often employ a divide-and-conquer strategy;
they divide their job by tools, work individually on separate tools, and
compile the results together in the end. In our study, we observed many
teams spontaneously conducted closer collaboration and enjoyed being
able to contribute simultaneously. A potential problem with simultaneous
contribution is duplicated or conflicted efforts. To minimize the
problem, further nuanced awareness features could be added. For example,
\autocite{Hajizadeh2013} revealed collaborator's action even though no
change is being made. We can visualize where the collaborator is making
an annotation as an indicator of interest of that piece of text. This is
like typing indicator bubbles in chatting tools (e.g.~Facebook
Messenger) which provides awareness of actions currently being performed
before outcome becomes visible.

A misconception about information analysis is that data modeling and
data analysis are two staged activities. This is akin to the waterfall
software development model, which features a sequential process that
flows downwards through the phases of requirement conception, software
design, software implementation, testing and maintenance. Critics have
pointed out that the staged approach may not work properly, because
clients may not know exactly what their requirements are before they see
the working software and designers may not be fully aware of future
difficulties in a new software product. Instead, an iterative design
process is often required that leads to reframe user requirements,
redesign, redevelopment, and retesting.

Similarly, relying only on information that has already been modeled and
delivered to analysts will probably not solve all analytical problems
\autocite{Heuer1999}. It will probably be necessary to look elsewhere
than evidence already extracted, re-model the data, and dig for more
information. Yet many analytic tools assume data has already been
modeled and ready to be visualized and analyzed and affords no utility
to construct or refine data models. As Ware termed as ``asymmetry in
data rates'' \autocite[382]{Ware2012}, analytic tools emphasized data
flowing from systems to users far more than from users to systems.
Functionalities are mostly designed to adjust visual representation
rather than remodel data underlying the representation, which is a
critical aspect in information analysis. Re-modeling of the data could
lead to a different picture of the problem (e.g.~adding a link between
two clusters changes the layout of the network, and thus framing of
relationships between two robberies), leading to completely different
analytic path. The interaction log we captured demonstrated such an
iterative analysis process and the positive subject feedback confirmed
data modeling as an integral part of analysis. This kind of problem
solving activity is better supported by an integrated workspace design
approach, rather than a workflow-driven design approach.

We noted the importance of representing uncertainty. We observed teams
in our study spontaneously employed two different approaches to deal
with team uncertainty: either to mix them for better synthesis or to
separate them for better clarification. This demonstrates both challenge
and opportunity to design for uncertainty support. We propose that a
richer graphic language and interaction be designed so that analysts can
encode uncertainty into the network view. For example, links and
entities with different uncertainty can be visualized in different
transparency. Users can \emph{filter} by uncertainty so that users can
choose to have only facts to take into account or review all inferences.

We noted several cases where teams created far more entities than needed
with an accretion strategy, and strikingly, also far more than that
reported in paper prototype studies \autocite{Carroll2013}. Why did this
happen? We guess both the context of classroom study and the system
design contributed. Unlike in the lab study where teams are temporarily
assembled, teams in a class evaluate peers either consciously or
unconsciously and value how themselves are being evaluated. Such social
pressure motivate individuals to make contributions, and indeed to make
\emph{visible} contributions, more than valuable contributions. That is,
participants noticed that their work activity was visible to their
partners, and accordingly prioritized doing more visible work over doing
less visible work. In some cases, this led to a new problem of easy and
less valuable contributions that were highly visible - such as creating
and therefore sharing data entities that were not particularly
important, and subsequently made data models seem cluttered. For
example, creating and therefore sharing an entity gets immediately
notified to the team whereas weighing the importance and relevance of an
entity goes silent in the system. We need to investigate approaches to
making significant contributions more visible, or perhaps making it more
immediately visible that less important contributions are indeed less
important.

Views could still get cluttered as data volume increases and analysts
dig into event details (e.g.~representing suspect's all actions to
identify patterns of common actions in two robberies). Indeed, analysts
often engage in multi-level analysis in parallel, frequently
coordinating among, say, confirmation of location of an event, to
comparison of two events, to overviewing all events as a robbery as a
whole. A possible design solution is to enable collapsible data views. A
collapsible view can help analysts focus attention on a certain level of
details at a time while conveniently switching between levels, and when
in collaboration, draw teammate's attention to a specific item.

An important improvement to CAnalytics is to support view sharing, or
sharing intermediate analytic status. A common solution is to have a
distinction of public view and private view \autocite{Convertino2011}.
We are proposing a step further that views should be treated as a team
\emph{resource}, just like data. Views as resource should be sharable,
extensible, and reusable. For example, several participants reflected
that there were situations when they found a collaborator's view
inspiring and wanted to continue exploring that view on their own side
without manually reproducing the view. With views as resource, the owner
of a view can deliberatively save the view as a shared resource when
they feel it appeals to collaborators. Other people can reuse the view
to their need. Shared views should be interactive rather than static
images, so that analysts can perform all interactions including
filtering and highlighting, and are able to evolve the view with
collective team efforts, a critical requirement emphasized in
\autocite{Carroll2013}.
